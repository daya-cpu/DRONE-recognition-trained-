# Step-by-step: Run your YOLOv11 model live in VS Code (webcam)

Below is a clear, follow-along checklist with exact commands and the complete `detect.py` you can copy-paste. I assume you're on **Windows** (your terminal showed `D:\DRONE>`), but I include activation notes for other shells.

---

## 1) Prep the project folder

1. Create a folder, e.g. `D:\DRONE\YOLOv11-Drone-Detection` and open it in VS Code.
2. Put your trained model `best.pt` inside that folder.

---

## 2) Create & activate a Python virtual environment

Open the VS Code terminal (Terminal → New Terminal) and run:

```powershell
# create venv
python -m venv venv
```

Activate it (pick the command that matches your shell):

* PowerShell:

```powershell
.\venv\Scripts\Activate.ps1
```

> If execution policy blocks this, run PowerShell as admin once and:
> `Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy RemoteSigned`

* Command Prompt (cmd):

```cmd
venv\Scripts\activate
```

* WSL / Git Bash / Mac / Linux:

```bash
source venv/bin/activate
```

You should see `(venv)` in the prompt.

---

## 3) Install dependencies

With the venv active, run:

```bash
pip install --upgrade pip
pip install ultralytics opencv-python matplotlib
```

---

## 4) Create the detection script (`detect.py`)

Create a file `detect.py` in the project folder and paste this complete script:

```python
import argparse
import os
from ultralytics import YOLO

# Helpful info about where the script runs
print("Working directory:", os.getcwd())

parser = argparse.ArgumentParser(description="Run YOLOv11 on webcam or file.")
parser.add_argument("--model", default="best.pt", help="Path to model weights")
parser.add_argument("--source", default="0", help="0 for default webcam or path to image/video")
parser.add_argument("--conf", type=float, default=0.5, help="Confidence threshold (0-1)")
parser.add_argument("--save", action="store_true", help="Save annotated output to runs/ directory")
args = parser.parse_args()

# convert numeric source to int for webcam
try:
    src = int(args.source)
except Exception:
    src = args.source

print(f"Loading model: {args.model}  |  Source: {src}  |  Conf: {args.conf}  |  Save: {args.save}")
model = YOLO(args.model)

# Run prediction (show=True opens a window)
results = model.predict(source=src, show=True, conf=args.conf, save=args.save)

# Optional: print brief info about detections
for i, r in enumerate(results):
    try:
        print(f"Frame {i}: {len(r.boxes)} boxes")
    except Exception:
        pass

print("Done. Annotated results (if saved) will be in runs/detect/...")
```

---

## 5) Run the script (webcam)

In the terminal (inside your project folder, venv active):

```bash
# default webcam, conf 0.5
python detect.py

# or explicitly:
python detect.py --source 0 --conf 0.6

# to save annotated video/images:
python detect.py --source 0 --conf 0.6 --save
```

* `--source 0` = default webcam. Try `--source 1` or `--source 2` if you have multiple cameras.
* Saved output goes to `runs/detect/predict/` (Ultralytics default).

---

## 6) Quick fixes & troubleshooting

* **If you see** `FileNotFoundError: test.jpg does not exist` → make sure `--source` points to `0` (webcam) or to an existing file; check `os.getcwd()` printed by the script.
* **Webcam busy or access error** → close apps using the camera (Zoom/Teams/OBS), try different camera index, or reboot.
* **Model file not found** → pass full path: `--model "C:\Users\...\best.pt"`
* **Face being detected as drone** → increase confidence:

  ```bash
  python detect.py --source 0 --conf 0.6
  ```

  Try `0.6`, `0.7`, etc., until false positives drop.
* **Headless / remote machine** → use `--save` and `show=False` (edit `show` in script) to avoid opening windows.

---

## 7) Useful extras (one-line tips)

* Filter by class index (if your model has multiple classes) — you can modify the predict call with `classes=[CLASS_INDEX]`:

```py
# inside detect.py, pass classes param if needed:
results = model.predict(source=src, show=True, conf=args.conf, save=args.save, classes=[0])
```

* Evaluate model performance:

```bash
yolo val model=best.pt data=data.yaml
```

* Export model to ONNX or TensorRT:

```bash
yolo export model=best.pt format=onnx
```

---

## 8) Add model info to README (paste this paragraph)

Add the following paragraph under **Model Information** in your README:

> The YOLOv11 model used in this project was trained for **50 epochs** on a **custom dataset containing over 40,000 images** of drones captured from various angles, lighting conditions, and backgrounds. The dataset included negative samples (non-drone images) to help the model distinguish drones from other objects and reduce false detections, improving generalization for real-time detection.

---

## 9) Next steps (if you want to improve accuracy)

* Add more drone images with varied backgrounds and distances.
* Add more negative images (faces, trees, vehicles) so the model learns what *isn't* a drone.
* Retrain or fine-tune from a pretrained YOLOv11 checkpoint.
* Use augmentation during training.
* Run `yolo val` to get precision/recall/mAP and spot weaknesses.

---

If you want, I can:

* Paste a shorter README formatted for copy-paste, **or**
* Generate a version of `detect.py` that launches a simple GUI to start/stop webcam.
  Which would you like next?
